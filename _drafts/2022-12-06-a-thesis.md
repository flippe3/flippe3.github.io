---
layout: post
title: "A Thesis"
tags: master, thesis, science, machine learning, studying, computer science
---

This is just a draft...

## Introduction
So I have to decide on a topic for my master's thesis, ever since high school, I've been extremely interested in the field of machine learning. I remember having a discussion with my dad in high school about what I should focus on. I'd done some game development and I'd come to realize that I wanted to do something else, something within computer science. I remember saying that I would either do hacking or machine learning, with machine learning being the winner since the future of hacking will probably involve machine learning.

I believe that I've studied at university for 4.5 years to specialize in a field and then try to produce something important for the world. This is what I want to do in my master's thesis, and perhaps I'm just delusional, and it's too little time to make any meaningful research, but I'd still want to try. If I fail miserably and need another 4 years to complete my thesis? Then that sounds like a good start for a PhD. 

## Natural Language Processing (NLP)
For the past 4.5 years I've been studying computer science at university, during the last year or so I've mostly spent my time focusing on machine learning. I've published two papers in NLP, and will probably co-author two more during the next few months. NLP is an exciting field and certainly an interesting one, earlier this year I was inspired by [InstructGPT](https://openai.com/blog/instruction-following/) and I even decided to try to create a smaller version of it for one of my courses with my friend. Then last week OpenAI released [ChatGPT](https://openai.com/blog/chatgpt/) a very similar project but accessible to the public with very impressive results. Making me wonder what the (inevitable) GPT-4 will look like. While NLP can get some crazy headlines and it is easy to praise our models for the impressive performance they've got, there is one major question that is still unanswered. 

*How should we teach our models to reason?* 

I suppose this is the general question of machine learning but it most certainly is a major question of NLP. We can output text but they can't think, well of course there is a happy bunch of "[Scale-Maximalists](https://www.lesswrong.com/posts/HzCFRvrHYFsCixWRs/raphael-milliere-on-generalization-and-scaling-maximalism)" that think scaling up our current transformers will give us something similar to human intelligence or perhaps beyond. I do not think this is the case and arguing about this would take up this whole point so I won't. So NLP has this interesting problem left to solve. We have large language models that can learn to reasonably predict what humans would write, especially so in the human-taught ChatGPT. But to me, the one vital problem that NLP lacks is thinking, where we get into a much more philosophical question, "How do humans think?", does our mind think in words and sentences? do we think in numbers? or perhaps pictures? 

This could very much be a completely irrelevant question, we learned how to fly without mimicking birds, so why should our machines mimic how we think? This is a valid point, perhaps our machines can think in probabilities of what words to say and this will eventually create an intelligent machine, or at least one that could imitate human intelligence, and what is to say that all humans aren't just faking it, mimicking our intelligence, what if consciousness is just what we invented to feel better about ourselves? Whatever. 

## Topic 1: Multi-Modal Embedding
I think that the human mind thinks in a special latent space, one that is neither words, images, numbers, or anything else. This brings me to my first topic, I'd like my master thesis to be about creating this latent space, a transformer that can learn from images, words, and numbers (equations) all at once. There should be no weird tokenization into different embeddings so that the model is just split up into three separate models but to build an embedding that could do all of them at once. This would be a kind of multi-modal embedding. So what will the research question be? What do I hope to be able to answer after 6 months of research?

"Is it possibele to create one embedding/neural network that can encode both images and text together in the same latent space to improve the generalization ability of the model?"

## Reinforcement Learning: 
Reinforcement learning was not a large part of machine learning until this British startup (Deepmind) created a [deep reinforcement learning agent](https://www.deepmind.com/publications/playing-atari-with-deep-reinforcement-learning) that could learn how to play atari by itself. Then suddenly the field of deep reinforcement learning (DRL) exploded. Now reinforcement learning is very much a significant part of the research published in machine learning. But if the brilliant transformers in NLP have problems surely RL must have problems too? Yes, "true" reinforcement learning is to drop an RL agent into an environment and let it learn by trial and error. It receives rewards from the environment and tries to use these rewards to guide it into solving any problem. Yet the problem is that RL often needs MILLIONS of tries before it learns how to play a game. Even though [AlphaGo Zero](https://www.deepmind.com/blog/alphago-zero-starting-from-scratch) was brilliant, it still didn't solve the problem of learning quickly it had to play against itself millions of times. Reinforcement learning is very similar to how brilliant minds before us thought about machine learning, famously Alan Turing wrote:
> Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child's? If this were then subjected to an appropriate course of education one would obtain the adult brain.

in his paper [COMPUTING MACHINERY AND INTELLIGENCE](https://redirect.cs.umbc.edu/courses/471/papers/turing.pdf) in 1950. I suppose we would disappoint Alan Turing even with all the brilliant achievements we've done, we still are no way near being able to teach a machine how to think with as little knowledge as humans do. Humans are extremely good at transfer learning, seeing our parents drive for hundreds of hours means that even though we've never driven a car we can learn it in a very short time like 50 hours or so. While reinforcement learning would have to drive and crash the car perhaps 10 million times before being able to keep the car on the road. 

So if reinforcement learning is so inefficient why would one be curious at all? Well, I see reinforcement learning mostly as an efficient search algorithm. Much like Deepmind proved that with DRL we could learn how to do matrix multiplication more efficiently with [AlphaTensor](https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor), this work mostly used Monte-Carlo Tree Search, much like Alphazero to search an extremely large search space rather efficiently. 

## Meta-Learning
So if RL is how we search for things, how will we teach our machines to think, that is when we bring in our next hype field. Meta-learning, which has probably mostly been misused as [Schmidhuber](https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber) talks about it. Most of the research in meta-learning seems to be very similar to transfer-learning, but the true essence of meta-learning is "Learning to learn", this means that we want to create a model? network? algorithm? that tries to learn how our model learns and then by knowing how it learns it could change the learning process or change the model itself to better understand new concepts. This sounds extremely interesting to me, it is one of the few things that humans have never been able to do, we've never been able to understand our brain. Then why would we make neural networks, machines that have the same fatal flaw???? Our machines should be able to understand their understanding and therefore improve themselves. Okay but this does sound very philosophical, how would one do this in practice and let alone in 6 months? 

## Topic 2: Meta-learning policy gradient
So the idea for this needs to be very clear because otherwise, this project will be really difficult to supervise, my idea is that I take three different games, let's say Chess, Atari, some card game (poker?), and do something similar to what [MuZero](https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules) does but with meta-learning instead of inverse RL. MuZero tries to learn without explicitly being told what the rules are, I'm more interested in creating a meta-learning system that can understand part of the learning process. I imagine this would be some kind of separate neural network that looks at how the RL system is learning how to play Chess, then it would try to apply that process when the agent learns to play Minecraft to speed up the system.

## Topic 3: Making transformers explainable
Currently, transformers are mostly black boxes they are just large probabilistic models that can't particularly explain how they found their output, what if we trained transformers to output not just text but from where they got most of their information? Then it would be easier to reference and read more into, it would be not only a good search engine but also easier to fact-check.

## Topic 4: [ARC](https://www.kaggle.com/c/abstraction-and-reasoning-challenge) & [ARC 2](https://arc-editor.lab42.global/)
ARC is a bunch of different puzzles that any human can almost immediately solve but is extremely hard for machines, none of the logic behind the puzzles is the same in any of them which makes it hard for a machine to learn how to do them. It was introduced by [Fran√ßois Chollet](https://en.wikipedia.org/wiki/Fran%C3%A7ois_Chollet) in his paper [On the Measure of Intelligence](https://arxiv.org/abs/1911.01547). The winner in the Kaggle competition did not use any sort of "Machine Learning" in the traditional sense and therefore I'm not sure how interested I am in working on this, it seems like it is extremely hard to design an algorithm general enough to be able to solve the puzzles, especially since there is not a large training-set available. The accuracy for the best result after ARC 1 being out for three years is probably below 40%, so nobody has found a good solution yet. ARC 2 is coming out next year though, and this would be easy to measure how well the research has succeeded.